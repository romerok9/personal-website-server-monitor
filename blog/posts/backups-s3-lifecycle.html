<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scripts de Backup: S3 + Lifecycle Policies | Kevin Romero</title>
    <meta name="description" content="Automatizaci√≥n de backups con AWS S3, pol√≠ticas de ciclo de vida, y scripts bash para mantener tus datos seguros sin costos excesivos.">
    <link rel="icon" type="image/svg+xml" href="/favicon.svg">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', sans-serif; background: #0a0a0a; color: #e0e0e0; line-height: 1.7; }
        .container { max-width: 800px; margin: 0 auto; padding: 2rem; }
        article { background: #111; border: 1px solid #222; padding: 3rem; border-radius: 8px; }
        .series-badge { background: #f59e0b20; color: #f59e0b; border: 1px solid #f59e0b; padding: 0.25rem 0.75rem; border-radius: 4px; font-size: 0.85rem; font-weight: 600; }
        h1 { font-size: 2.5rem; color: #fff; margin: 1rem 0; }
        h2 { color: #fff; font-size: 1.8rem; margin: 2rem 0 1rem; border-bottom: 2px solid #333; padding-bottom: 0.5rem; }
        h3 { color: #e0e0e0; font-size: 1.3rem; margin: 1.5rem 0 1rem; }
        pre { background: #0d0d0d; border: 1px solid #222; padding: 1.5rem; border-radius: 6px; overflow-x: auto; margin: 1.5rem 0; }
        code { background: #1a1a1a; color: #60a5fa; padding: 0.2rem 0.5rem; border-radius: 3px; font-family: 'Courier New', monospace; }
        pre code { background: none; padding: 0; color: #d0d0d0; }
        .info-box { background: #0d1b2a; border-left: 4px solid #60a5fa; padding: 1rem 1.5rem; margin: 1.5rem 0; }
        .success-box { background: #0d2a1b; border-left: 4px solid #10b981; padding: 1rem 1.5rem; margin: 1.5rem 0; }
        .warning-box { background: #2a1a0d; border-left: 4px solid #f59e0b; padding: 1rem 1.5rem; margin: 1.5rem 0; }
        ul, ol { margin-left: 2rem; margin-top: 1rem; }
        li { margin: 0.5rem 0; }
        table { width: 100%; border-collapse: collapse; margin: 1.5rem 0; }
        th, td { padding: 0.75rem; text-align: left; border: 1px solid #222; }
        th { background: #1a1a1a; color: #60a5fa; }
        footer { margin-top: 3rem; padding-top: 2rem; border-top: 1px solid #333; text-align: center; color: #666; }
    </style>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-19H0L3L7QB"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-19H0L3L7QB');
</script>

</head>
<body>
    <div class="container">
        <a href="/blog/" style="color: #60a5fa; text-decoration: none;">‚Üê Back to Blog</a>
        <article>
            <div style="margin-bottom: 2rem; padding-bottom: 2rem; border-bottom: 1px solid #222;">
                <span class="series-badge">‚òÅÔ∏è Automatizaci√≥n AWS</span>
                <div style="color: #666; margin: 0.5rem 0;">December 5, 2025</div>
                <h1>Scripts de Backup: S3 + Lifecycle Policies</h1>
            </div>
            
            <div style="color: #d0d0d0;">
                <h2>üìã Contexto</h2>
                <p>Los backups son como los seguros: nadie quiere pagarlos hasta que los necesita. En este post, c√≥mo implement√© un sistema de backups automatizado para mi home lab usando S3, con lifecycle policies para optimizar costos y scripts bash para automatizaci√≥n completa.</p>

                <h2>üéØ Estrategia de Backup (3-2-1 Rule)</h2>
                <ul>
                    <li><strong>3 copias:</strong> Original + 2 backups</li>
                    <li><strong>2 medios diferentes:</strong> Disco local + cloud (S3)</li>
                    <li><strong>1 copia offsite:</strong> S3 en regi√≥n diferente</li>
                </ul>

                <h2>üí∞ Optimizaci√≥n de Costos con S3 Storage Classes</h2>
                <table>
                    <tr>
                        <th>Storage Class</th>
                        <th>Costo (GB/mes)</th>
                        <th>Uso</th>
                    </tr>
                    <tr>
                        <td>S3 Standard</td>
                        <td>$0.023</td>
                        <td>√öltimos 7 d√≠as</td>
                    </tr>
                    <tr>
                        <td>S3 Standard-IA</td>
                        <td>$0.0125</td>
                        <td>8-30 d√≠as</td>
                    </tr>
                    <tr>
                        <td>S3 Glacier Flexible</td>
                        <td>$0.0036</td>
                        <td>31-90 d√≠as</td>
                    </tr>
                    <tr>
                        <td>S3 Glacier Deep Archive</td>
                        <td>$0.00099</td>
                        <td>>90 d√≠as</td>
                    </tr>
                </table>

                <div class="info-box">
                    <strong>üí° Ahorro:</strong> Mover backups de 30 d√≠as de Standard a Glacier = ~84% menos costo
                </div>

                <h2>üõ†Ô∏è Script de Backup Completo</h2>
                <pre><code>#!/bin/bash
# backup-to-s3.sh - Automated backup script with lifecycle management

set -euo pipefail
IFS=$'\n\t'

##############################################################################
# Configuration
##############################################################################

readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly BACKUP_NAME="homelab-backup"
readonly S3_BUCKET="my-backups-$(aws sts get-caller-identity --query Account --output text)"
readonly S3_PREFIX="homelab/"
readonly BACKUP_DIRS=(
    "/home/mytechzone/n8n-lab"
    "/home/mytechzone/scripts"
    "/etc/nginx"
    "/etc/systemd/system"
)
readonly EXCLUDE_PATTERNS=(
    "*.log"
    "*.tmp"
    "node_modules/"
    ".git/"
    "__pycache__/"
)
readonly TEMP_DIR="/tmp/backups"
readonly LOG_FILE="/var/log/backup-s3.log"
readonly RETENTION_DAYS=90

##############################################################################
# Logging
##############################################################################

log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] [INFO] $*" | tee -a "$LOG_FILE"
}

error() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] [ERROR] $*" | tee -a "$LOG_FILE" >&2
}

##############################################################################
# Validation
##############################################################################

check_prerequisites() {
    log "Checking prerequisites..."
    
    # Check AWS CLI
    if ! command -v aws &> /dev/null; then
        error "AWS CLI not installed"
        exit 1
    fi
    
    # Check authentication
    if ! aws sts get-caller-identity &> /dev/null; then
        error "Not authenticated with AWS"
        exit 1
    fi
    
    # Check disk space (need at least 5GB free)
    local available=$(df /tmp | tail -1 | awk '{print $4}')
    if [ "$available" -lt 5242880 ]; then
        error "Not enough disk space in /tmp"
        exit 1
    fi
    
    log "‚úì Prerequisites OK"
}

##############################################################################
# S3 Bucket Setup
##############################################################################

setup_s3_bucket() {
    log "Setting up S3 bucket: $S3_BUCKET"
    
    # Create bucket if doesn't exist
    if ! aws s3 ls "s3://$S3_BUCKET" &> /dev/null; then
        log "Creating bucket..."
        aws s3 mb "s3://$S3_BUCKET" --region us-east-1
        
        # Enable versioning
        aws s3api put-bucket-versioning \
            --bucket "$S3_BUCKET" \
            --versioning-configuration Status=Enabled
        
        # Enable encryption
        aws s3api put-bucket-encryption \
            --bucket "$S3_BUCKET" \
            --server-side-encryption-configuration '{
                "Rules": [{
                    "ApplyServerSideEncryptionByDefault": {
                        "SSEAlgorithm": "AES256"
                    }
                }]
            }'
    fi
    
    log "‚úì Bucket ready"
}

##############################################################################
# Lifecycle Policy
##############################################################################

configure_lifecycle() {
    log "Configuring lifecycle policy..."
    
    cat > /tmp/lifecycle-policy.json << EOF
{
    "Rules": [
        {
            "Id": "BackupRetention",
            "Status": "Enabled",
            "Prefix": "$S3_PREFIX",
            "Transitions": [
                {
                    "Days": 7,
                    "StorageClass": "STANDARD_IA"
                },
                {
                    "Days": 30,
                    "StorageClass": "GLACIER"
                },
                {
                    "Days": 90,
                    "StorageClass": "DEEP_ARCHIVE"
                }
            ],
            "Expiration": {
                "Days": $RETENTION_DAYS
            }
        }
    ]
}
EOF
    
    aws s3api put-bucket-lifecycle-configuration \
        --bucket "$S3_BUCKET" \
        --lifecycle-configuration file:///tmp/lifecycle-policy.json
    
    rm /tmp/lifecycle-policy.json
    
    log "‚úì Lifecycle configured"
}

##############################################################################
# Create Backup
##############################################################################

create_backup() {
    log "Creating backup archive..."
    
    mkdir -p "$TEMP_DIR"
    
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="$TEMP_DIR/${BACKUP_NAME}_${timestamp}.tar.gz"
    
    # Build exclude options
    local exclude_opts=""
    for pattern in "${EXCLUDE_PATTERNS[@]}"; do
        exclude_opts+=" --exclude='$pattern'"
    done
    
    # Create tar archive
    log "Archiving directories..."
    eval tar czf "$backup_file" \
        $exclude_opts \
        "${BACKUP_DIRS[@]}" \
        2>> "$LOG_FILE" || {
            error "Tar failed"
            return 1
        }
    
    # Get size
    local size_mb=$(du -m "$backup_file" | cut -f1)
    log "‚úì Backup created: $backup_file ($size_mb MB)"
    
    echo "$backup_file"
}

##############################################################################
# Upload to S3
##############################################################################

upload_to_s3() {
    local backup_file=$1
    local filename=$(basename "$backup_file")
    local s3_path="s3://$S3_BUCKET/$S3_PREFIX$filename"
    
    log "Uploading to S3: $s3_path"
    
    # Upload with progress
    aws s3 cp "$backup_file" "$s3_path" \
        --storage-class STANDARD \
        --metadata "backup-date=$(date -Iseconds),hostname=$(hostname)" \
        2>&1 | tee -a "$LOG_FILE"
    
    if [ ${PIPESTATUS[0]} -eq 0 ]; then
        log "‚úì Upload successful"
        return 0
    else
        error "Upload failed"
        return 1
    fi
}

##############################################################################
# Cleanup Old Backups (Local)
##############################################################################

cleanup_local() {
    log "Cleaning up local temp files..."
    
    find "$TEMP_DIR" -name "${BACKUP_NAME}_*.tar.gz" -type f -mtime +7 -delete
    
    log "‚úì Local cleanup done"
}

##############################################################################
# Verify Backup
##############################################################################

verify_backup() {
    local s3_path=$1
    
    log "Verifying backup in S3..."
    
    # Check if file exists
    if aws s3 ls "$s3_path" &> /dev/null; then
        log "‚úì Backup verified in S3"
        return 0
    else
        error "Backup not found in S3"
        return 1
    fi
}

##############################################################################
# Notification (Optional)
##############################################################################

send_notification() {
    local status=$1
    local message=$2
    
    # Option 1: Email via SES
    # aws ses send-email ...
    
    # Option 2: SNS
    # aws sns publish --topic-arn "..." --message "$message"
    
    # Option 3: Webhook (Discord, Slack)
    # curl -X POST -H 'Content-type: application/json' \
    #      --data "{\"text\":\"$message\"}" \
    #      "$WEBHOOK_URL"
    
    log "Notification sent: $status"
}

##############################################################################
# Main
##############################################################################

main() {
    log "=========================================="
    log "Starting backup process"
    log "=========================================="
    
    # Validate
    check_prerequisites
    
    # Setup infrastructure
    setup_s3_bucket
    configure_lifecycle
    
    # Create and upload backup
    local backup_file
    if backup_file=$(create_backup); then
        if upload_to_s3 "$backup_file"; then
            local s3_path="s3://$S3_BUCKET/$S3_PREFIX$(basename "$backup_file")"
            
            if verify_backup "$s3_path"; then
                log "‚úÖ Backup completed successfully"
                send_notification "SUCCESS" "Backup completed: $(basename "$backup_file")"
                
                # Cleanup
                cleanup_local
                rm -f "$backup_file"
                
                exit 0
            fi
        fi
    fi
    
    error "‚ùå Backup failed"
    send_notification "FAILURE" "Backup failed - check logs"
    exit 1
}

# Run
main "$@"</code></pre>

                <h2>‚è∞ Automatizaci√≥n con Cron</h2>
                <pre><code># /etc/cron.d/backup-s3
# Run backup daily at 2 AM
0 2 * * * mytechzone /home/mytechzone/scripts/backup-to-s3.sh

# Weekly full backup on Sunday
0 3 * * 0 mytechzone /home/mytechzone/scripts/backup-to-s3.sh --full

# Monthly deep backup (1st day of month)
0 4 1 * * mytechzone /home/mytechzone/scripts/backup-to-s3.sh --monthly</code></pre>

                <h2>üìä Script de Restore</h2>
                <pre><code>#!/bin/bash
# restore-from-s3.sh - Restore backup from S3

set -euo pipefail

readonly S3_BUCKET="my-backups-123456789012"
readonly S3_PREFIX="homelab/"
readonly RESTORE_DIR="/tmp/restore"

list_backups() {
    echo "Available backups:"
    echo ""
    
    aws s3 ls "s3://$S3_BUCKET/$S3_PREFIX" \
        --recursive \
        --human-readable \
        --summarize | \
    grep -E '\.tar\.gz$' | \
    nl -w2 -s'. '
}

download_backup() {
    local backup_file=$1
    local s3_path="s3://$S3_BUCKET/$S3_PREFIX$backup_file"
    local local_path="$RESTORE_DIR/$backup_file"
    
    echo "Downloading $backup_file..."
    
    mkdir -p "$RESTORE_DIR"
    
    aws s3 cp "$s3_path" "$local_path" \
        --storage-class STANDARD  # Restore from Glacier if needed
    
    echo "‚úì Downloaded to: $local_path"
}

restore_backup() {
    local backup_file=$1
    local target_dir=${2:-/}
    
    echo "Restoring to: $target_dir"
    
    tar xzf "$RESTORE_DIR/$backup_file" -C "$target_dir" --verbose
    
    echo "‚úì Restore complete"
}

main() {
    if [ $# -eq 0 ]; then
        list_backups
        echo ""
        read -p "Enter backup filename to restore: " backup_file
    else
        backup_file=$1
    fi
    
    download_backup "$backup_file"
    
    read -p "Restore to / ? (yes/no): " confirm
    if [ "$confirm" = "yes" ]; then
        restore_backup "$backup_file"
    else
        echo "Backup downloaded to $RESTORE_DIR/$backup_file"
        echo "Extract manually with: tar xzf $RESTORE_DIR/$backup_file -C /target/path"
    fi
}

main "$@"</code></pre>

                <h2>üí° Monitoring de Backups</h2>
                <pre><code>#!/bin/bash
# check-backup-status.sh - Monitor backup health

set -euo pipefail

readonly S3_BUCKET="my-backups-123456789012"
readonly S3_PREFIX="homelab/"
readonly MAX_AGE_HOURS=28  # Alert if no backup in 28 hours

get_latest_backup() {
    aws s3 ls "s3://$S3_BUCKET/$S3_PREFIX" \
        --recursive | \
    grep -E '\.tar\.gz$' | \
    sort -k1,2 | \
    tail -1 | \
    awk '{print $1, $2, $4}'
}

check_backup_age() {
    local latest=$(get_latest_backup)
    
    if [ -z "$latest" ]; then
        echo "‚ùå No backups found!"
        return 1
    fi
    
    local backup_date=$(echo "$latest" | awk '{print $1, $2}')
    local backup_file=$(echo "$latest" | awk '{print $3}')
    local backup_timestamp=$(date -d "$backup_date" +%s)
    local current_timestamp=$(date +%s)
    local age_hours=$(( (current_timestamp - backup_timestamp) / 3600 ))
    
    echo "Latest backup: $backup_file"
    echo "Age: $age_hours hours"
    
    if [ $age_hours -gt $MAX_AGE_HOURS ]; then
        echo "‚ö†Ô∏è  Backup is too old!"
        return 1
    fi
    
    echo "‚úÖ Backup is current"
    return 0
}

main() {
    echo "=== Backup Status Check ==="
    echo ""
    check_backup_age
}

main "$@"</code></pre>

                <h2>üìà Estimaci√≥n de Costos</h2>
                <pre><code># Escenario: 10 GB backup diario, retention 90 d√≠as

# Month 1:
Days 1-7:   70 GB √ó $0.023  = $1.61
Days 8-30:  230 GB √ó $0.0125 = $2.88
Days 31+:   0 GB (no data yet)
Total: $4.49

# Month 2:
Days 1-7:   70 GB √ó $0.023  = $1.61
Days 8-30:  230 GB √ó $0.0125 = $2.88
Days 31-60: 310 GB √ó $0.0036 = $1.12
Total: $5.61

# Month 3 (steady state):
Days 1-7:   70 GB √ó $0.023  = $1.61
Days 8-30:  230 GB √ó $0.0125 = $2.88
Days 31-90: 610 GB √ó $0.0036 = $2.20
Total: $6.69/month

# vs keeping everything in S3 Standard:
900 GB √ó $0.023 = $20.70/month
Savings: ~68%</code></pre>

                <div class="success-box">
                    <strong>‚úÖ Ahorro Real:</strong> De $20/mes a $7/mes = $156/a√±o ahorrados con lifecycle policies
                </div>

                <h2>üö® Testing del Restore</h2>
                <pre><code>#!/bin/bash
# test-restore.sh - Automated restore testing

set -euo pipefail

readonly TEST_DIR="/tmp/restore-test"
readonly S3_BUCKET="my-backups-123456789012"
readonly S3_PREFIX="homelab/"

test_restore() {
    echo "=== Backup Restore Test ==="
    echo ""
    
    # Get latest backup
    local latest_backup=$(aws s3 ls "s3://$S3_BUCKET/$S3_PREFIX" \
        --recursive | \
        grep -E '\.tar\.gz$' | \
        sort | \
        tail -1 | \
        awk '{print $4}')
    
    echo "Testing: $latest_backup"
    
    # Download
    mkdir -p "$TEST_DIR"
    aws s3 cp "s3://$S3_BUCKET/$latest_backup" "$TEST_DIR/"
    
    # Extract
    tar tzf "$TEST_DIR/$(basename "$latest_backup")" &> /dev/null
    
    if [ $? -eq 0 ]; then
        echo "‚úÖ Backup is valid and can be restored"
        rm -rf "$TEST_DIR"
        return 0
    else
        echo "‚ùå Backup is corrupted!"
        return 1
    fi
}

# Run monthly via cron
test_restore</code></pre>

                <h2>üí° Best Practices</h2>
                <ul>
                    <li>‚úÖ <strong>Test restores regularmente:</strong> Un backup sin test es un backup sin valor</li>
                    <li>‚úÖ <strong>Encripci√≥n:</strong> Usa S3 SSE o KMS</li>
                    <li>‚úÖ <strong>Versioning:</strong> Enable S3 versioning para protecci√≥n contra deletes accidentales</li>
                    <li>‚úÖ <strong>Multi-region:</strong> Replica backups cr√≠ticos a otra regi√≥n</li>
                    <li>‚úÖ <strong>Alertas:</strong> Notificaciones si backup falla</li>
                    <li>‚úÖ <strong>Lifecycle policies:</strong> Optimiza costos autom√°ticamente</li>
                    <li>‚úÖ <strong>Exclude unnecessary files:</strong> logs, tmp, node_modules</li>
                    <li>‚ùå <strong>No hardcodear credenciales:</strong> Usa IAM roles o SSO</li>
                </ul>

                <h2>üîê IAM Policy Necesaria</h2>
                <pre><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::my-backups-*",
                "arn:aws:s3:::my-backups-*/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:CreateBucket",
                "s3:PutBucketVersioning",
                "s3:PutLifecycleConfiguration"
            ],
            "Resource": "arn:aws:s3:::my-backups-*"
        }
    ]
}</code></pre>

                <h2>üìä Mi Resultado Final</h2>
                <div class="success-box">
                    <strong>Infrastructure:</strong><br>
                    ‚Ä¢ Backup diario autom√°tico a las 2 AM<br>
                    ‚Ä¢ Retention: 90 d√≠as<br>
                    ‚Ä¢ Tama√±o promedio: 8 GB/backup<br>
                    ‚Ä¢ Storage classes: Standard ‚Üí IA ‚Üí Glacier<br><br>
                    
                    <strong>Costs:</strong><br>
                    ‚Ä¢ $6.50/mes (~720 GB total)<br>
                    ‚Ä¢ vs $18/mes sin lifecycle<br>
                    ‚Ä¢ Ahorro: 64%<br><br>
                    
                    <strong>Reliability:</strong><br>
                    ‚Ä¢ 180+ backups exitosos consecutivos<br>
                    ‚Ä¢ 0 fallos en 6 meses<br>
                    ‚Ä¢ Restore testing: mensual, 100% √©xito<br>
                    ‚Ä¢ Recovery time: ~10 minutos
                </div>

                <h2>üìö Recursos</h2>
                <ul>
                    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html" target="_blank">S3 Lifecycle Management</a></li>
                    <li><a href="https://aws.amazon.com/s3/storage-classes/" target="_blank">S3 Storage Classes</a></li>
                    <li><a href="https://calculator.aws/" target="_blank">AWS Pricing Calculator</a></li>
                </ul>

                <h2>üí≠ Conclusi√≥n</h2>
                <p>Los backups no tienen que ser caros ni complicados. Con S3 + lifecycle policies + un buen script bash, tienes un sistema enterprise-grade por menos de $10/mes. La clave es automatizaci√≥n + testing regular.</p>

                <p>Recuerda: el mejor backup es el que funciona cuando lo necesitas. Testea tus restores. Siempre.</p>
            </div>
        </article>
        <footer>
            <p>¬© 2025-2026 Kevin Romero | <a href="/">Home</a> | <a href="/blog/">Blog</a></p>
        </footer>
    </div>
</body>
</html>

